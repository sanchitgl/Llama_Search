{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 350\n",
    "model_name = \"distilbert-base-uncased\" \n",
    "dataset = \"scifact\"\n",
    "data_path = f\"../beir/datasets/{dataset}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import losses, models, SentenceTransformer\n",
    "from beir import util, LoggingHandler\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.search.lexical import BM25Search as BM25\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "from beir.retrieval.train import TrainRetriever\n",
    "import pathlib, os, tqdm\n",
    "import logging\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate-distilBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_save_path = os.path.join(\"../\", \"output\", \"{}-v1-{}\".format(model_name, dataset)) \n",
    "\n",
    "# R: os.path.join(\"../\", \"output\", \"{}-v1-{}\".format(model_name, dataset)) \n",
    "# BM: os.path.join(os.getcwd(), \"../output\", \"{}-v2-{}-bm25-hard-negs\".format(model_name, dataset)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-02 21:00:48 - Loading Corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5183/5183 [00:00<00:00, 129640.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-02 21:00:48 - Loaded 5183 TEST Documents.\n",
      "2024-06-02 21:00:48 - Doc Example: {'text': 'Alterations of the architecture of cerebral white matter in the developing human brain can affect cortical development and result in functional disabilities. A line scan diffusion-weighted magnetic resonance imaging (MRI) sequence with diffusion tensor analysis was applied to measure the apparent diffusion coefficient, to calculate relative anisotropy, and to delineate three-dimensional fiber architecture in cerebral white matter in preterm (n = 17) and full-term infants (n = 7). To assess effects of prematurity on cerebral white matter development, early gestation preterm infants (n = 10) were studied a second time at term. In the central white matter the mean apparent diffusion coefficient at 28 wk was high, 1.8 microm2/ms, and decreased toward term to 1.2 microm2/ms. In the posterior limb of the internal capsule, the mean apparent diffusion coefficients at both times were similar (1.2 versus 1.1 microm2/ms). Relative anisotropy was higher the closer birth was to term with greater absolute values in the internal capsule than in the central white matter. Preterm infants at term showed higher mean diffusion coefficients in the central white matter (1.4 +/- 0.24 versus 1.15 +/- 0.09 microm2/ms, p = 0.016) and lower relative anisotropy in both areas compared with full-term infants (white matter, 10.9 +/- 0.6 versus 22.9 +/- 3.0%, p = 0.001; internal capsule, 24.0 +/- 4.44 versus 33.1 +/- 0.6% p = 0.006). Nonmyelinated fibers in the corpus callosum were visible by diffusion tensor MRI as early as 28 wk; full-term and preterm infants at term showed marked differences in white matter fiber organization. The data indicate that quantitative assessment of water diffusion by diffusion tensor MRI provides insight into microstructural development in cerebral white matter in living infants.', 'title': 'Microstructural development of human newborn cerebral white matter assessed in vivo by diffusion tensor magnetic resonance imaging.'}\n",
      "2024-06-02 21:00:48 - Loading Queries...\n",
      "2024-06-02 21:00:48 - Loaded 300 TEST Queries.\n",
      "2024-06-02 21:00:48 - Query Example: 0-dimensional biomaterials show inductive properties.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading test set\n",
    "corpus, queries, qrels = GenericDataLoader(data_path).load(split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-02 21:06:59 - Use pytorch device_name: cuda\n",
      "2024-06-02 21:06:59 - Load pretrained SentenceTransformer: ../output/distilbert-base-uncased-v1-scifact\n",
      "2024-06-02 21:07:01 - Encoding Queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-02 21:07:01 - Sorting Corpus by document length (Longest first)...\n",
      "2024-06-02 21:07:01 - Encoding Corpus in batches... Warning: This might take a while!\n",
      "2024-06-02 21:07:01 - Scoring Function: Cosine Similarity (cos_sim)\n",
      "2024-06-02 21:07:01 - Encoding Batch 1/1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 41/41 [00:10<00:00,  3.94it/s]\n"
     ]
    }
   ],
   "source": [
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "from beir.retrieval import models\n",
    "from beir.retrieval.search.dense import DenseRetrievalExactSearch as DRES\n",
    "\n",
    "## Load retriever from saved model\n",
    "\n",
    "model = DRES(models.SentenceBERT(model_save_path), batch_size=128)\n",
    "retriever = EvaluateRetrieval(model, score_function=\"cos_sim\")\n",
    "\n",
    "#### Retrieve dense results (format of results is identical to qrels)\n",
    "results = retriever.retrieve(corpus, queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# output_path = \"../output/\"\n",
    "# with open(f\"{output_path}{dataset}_distilBert_results.json\", 'w') as f:\n",
    "#     json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# output_path = \"../output/\"\n",
    "# with open(f\"{output_path}{dataset}_distilBert_results.json\", 'r') as f:\n",
    "#     results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-02 21:07:22 - Retriever evaluation for k in: [1, 3, 5, 10, 100, 1000]\n",
      "2024-06-02 21:07:22 - For evaluation, we ignore identical query and document ids (default), please explicitly set ``ignore_identical_ids=False`` to ignore this.\n",
      "2024-06-02 21:07:22 - \n",
      "\n",
      "2024-06-02 21:07:22 - NDCG@1: 0.5000\n",
      "2024-06-02 21:07:22 - NDCG@3: 0.5845\n",
      "2024-06-02 21:07:22 - NDCG@5: 0.6121\n",
      "2024-06-02 21:07:22 - NDCG@10: 0.6354\n",
      "2024-06-02 21:07:22 - NDCG@100: 0.6641\n",
      "2024-06-02 21:07:22 - NDCG@1000: 0.6725\n",
      "2024-06-02 21:07:22 - \n",
      "\n",
      "2024-06-02 21:07:22 - MAP@1: 0.4772\n",
      "2024-06-02 21:07:22 - MAP@3: 0.5551\n",
      "2024-06-02 21:07:22 - MAP@5: 0.5741\n",
      "2024-06-02 21:07:22 - MAP@10: 0.5847\n",
      "2024-06-02 21:07:22 - MAP@100: 0.5912\n",
      "2024-06-02 21:07:22 - MAP@1000: 0.5915\n",
      "2024-06-02 21:07:22 - \n",
      "\n",
      "2024-06-02 21:07:22 - Recall@1: 0.4772\n",
      "2024-06-02 21:07:22 - Recall@3: 0.6464\n",
      "2024-06-02 21:07:22 - Recall@5: 0.7123\n",
      "2024-06-02 21:07:22 - Recall@10: 0.7797\n",
      "2024-06-02 21:07:22 - Recall@100: 0.9073\n",
      "2024-06-02 21:07:22 - Recall@1000: 0.9767\n",
      "2024-06-02 21:07:22 - \n",
      "\n",
      "2024-06-02 21:07:22 - P@1: 0.5000\n",
      "2024-06-02 21:07:22 - P@3: 0.2322\n",
      "2024-06-02 21:07:22 - P@5: 0.1573\n",
      "2024-06-02 21:07:22 - P@10: 0.0873\n",
      "2024-06-02 21:07:22 - P@100: 0.0102\n",
      "2024-06-02 21:07:22 - P@1000: 0.0011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'NDCG@1': 0.5,\n",
       "  'NDCG@3': 0.58448,\n",
       "  'NDCG@5': 0.61214,\n",
       "  'NDCG@10': 0.63538,\n",
       "  'NDCG@100': 0.66406,\n",
       "  'NDCG@1000': 0.67247},\n",
       " {'MAP@1': 0.47722,\n",
       "  'MAP@3': 0.55514,\n",
       "  'MAP@5': 0.57411,\n",
       "  'MAP@10': 0.58474,\n",
       "  'MAP@100': 0.59124,\n",
       "  'MAP@1000': 0.59148},\n",
       " {'Recall@1': 0.47722,\n",
       "  'Recall@3': 0.64639,\n",
       "  'Recall@5': 0.71233,\n",
       "  'Recall@10': 0.77967,\n",
       "  'Recall@100': 0.90733,\n",
       "  'Recall@1000': 0.97667},\n",
       " {'P@1': 0.5,\n",
       "  'P@3': 0.23222,\n",
       "  'P@5': 0.15733,\n",
       "  'P@10': 0.08733,\n",
       "  'P@100': 0.01023,\n",
       "  'P@1000': 0.00111})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Evaluate your retrieval using NDCG@k, MAP@K ...\n",
    "logging.info(\"Retriever evaluation for k in: {}\".format(retriever.k_values))\n",
    "ndcg, _map, recall, precision = retriever.evaluate(qrels, results, retriever.k_values)\n",
    "ndcg, _map, recall, precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../beir/datasets/scifact'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BM25 scores\n",
    "import pickle \n",
    "\n",
    "with open(f\"{data_path}/{dataset}_bm25_scores.pickle\", 'rb') as f:\n",
    "    results_bm25 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-02 21:17:13 - Activating Elasticsearch....\n",
      "2024-06-02 21:17:13 - Elastic Search Credentials: {'hostname': 'localhost', 'index_name': 'scifact_2', 'keys': {'title': 'title', 'body': 'txt'}, 'timeout': 100, 'retry_on_timeout': True, 'maxsize': 24, 'number_of_shards': 'default', 'language': 'english'}\n",
      "2024-06-02 21:17:13 - Deleting previous Elasticsearch-Index named - scifact_2\n",
      "2024-06-02 21:17:13 - Unable to create Index in Elastic Search. Reason: ConnectionError(('Connection aborted.', BadStatusLine('ÿ\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x7fe\\x00tity\\r\\n'))) caused by: ProtocolError(('Connection aborted.', BadStatusLine('ÿ\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x7fe\\x00tity\\r\\n')))\n",
      "2024-06-02 21:17:15 - Creating fresh Elasticsearch-Index named - scifact_2\n",
      "2024-06-02 21:17:15 - Unable to create Index in Elastic Search. Reason: ConnectionError(('Connection aborted.', BadStatusLine('ÿ\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x7f-\\x00ength: 117\\r\\n'))) caused by: ProtocolError(('Connection aborted.', BadStatusLine('ÿ\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x7f-\\x00ength: 117\\r\\n')))\n"
     ]
    }
   ],
   "source": [
    "from beir.retrieval.search.lexical import BM25Search as BM25\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "\n",
    "## elasticsearch settings\n",
    "hostname = \"localhost\" #localhost\n",
    "index_name = dataset+'_2' # scifact\n",
    "initialize = True # True - Delete existing index and re-index all documents from scratch \n",
    "\n",
    "model_bm25 = BM25(index_name=index_name, hostname=hostname, initialize=initialize)\n",
    "retriever_bm25 = EvaluateRetrieval(model_bm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-02 21:17:18 - For evaluation, we ignore identical query and document ids (default), please explicitly set ``ignore_identical_ids=False`` to ignore this.\n",
      "2024-06-02 21:17:18 - \n",
      "\n",
      "2024-06-02 21:17:18 - NDCG@1: 0.5767\n",
      "2024-06-02 21:17:18 - NDCG@3: 0.6366\n",
      "2024-06-02 21:17:18 - NDCG@5: 0.6652\n",
      "2024-06-02 21:17:18 - NDCG@10: 0.6906\n",
      "2024-06-02 21:17:18 - NDCG@100: 0.7134\n",
      "2024-06-02 21:17:18 - NDCG@1000: 0.7212\n",
      "2024-06-02 21:17:18 - \n",
      "\n",
      "2024-06-02 21:17:18 - MAP@1: 0.5559\n",
      "2024-06-02 21:17:18 - MAP@3: 0.6143\n",
      "2024-06-02 21:17:18 - MAP@5: 0.6312\n",
      "2024-06-02 21:17:18 - MAP@10: 0.6437\n",
      "2024-06-02 21:17:18 - MAP@100: 0.6492\n",
      "2024-06-02 21:17:18 - MAP@1000: 0.6495\n",
      "2024-06-02 21:17:18 - \n",
      "\n",
      "2024-06-02 21:17:18 - Recall@1: 0.5559\n",
      "2024-06-02 21:17:18 - Recall@3: 0.6793\n",
      "2024-06-02 21:17:18 - Recall@5: 0.7479\n",
      "2024-06-02 21:17:18 - Recall@10: 0.8198\n",
      "2024-06-02 21:17:18 - Recall@100: 0.9192\n",
      "2024-06-02 21:17:18 - Recall@1000: 0.9800\n",
      "2024-06-02 21:17:18 - \n",
      "\n",
      "2024-06-02 21:17:18 - P@1: 0.5767\n",
      "2024-06-02 21:17:18 - P@3: 0.2411\n",
      "2024-06-02 21:17:18 - P@5: 0.1620\n",
      "2024-06-02 21:17:18 - P@10: 0.0907\n",
      "2024-06-02 21:17:18 - P@100: 0.0104\n",
      "2024-06-02 21:17:18 - P@1000: 0.0011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'NDCG@1': 0.57667,\n",
       "  'NDCG@3': 0.63658,\n",
       "  'NDCG@5': 0.66524,\n",
       "  'NDCG@10': 0.69064,\n",
       "  'NDCG@100': 0.71337,\n",
       "  'NDCG@1000': 0.7212},\n",
       " {'MAP@1': 0.55594,\n",
       "  'MAP@3': 0.61432,\n",
       "  'MAP@5': 0.63124,\n",
       "  'MAP@10': 0.64374,\n",
       "  'MAP@100': 0.64918,\n",
       "  'MAP@1000': 0.6495},\n",
       " {'Recall@1': 0.55594,\n",
       "  'Recall@3': 0.67928,\n",
       "  'Recall@5': 0.74789,\n",
       "  'Recall@10': 0.81978,\n",
       "  'Recall@100': 0.91922,\n",
       "  'Recall@1000': 0.98},\n",
       " {'P@1': 0.57667,\n",
       "  'P@3': 0.24111,\n",
       "  'P@5': 0.162,\n",
       "  'P@10': 0.09067,\n",
       "  'P@100': 0.0104,\n",
       "  'P@1000': 0.00111})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Evaluate your retrieval using NDCG@k, MAP@K ...\n",
    "ndcg, _map, recall, precision = retriever_bm25.evaluate(qrels, results_bm25, retriever_bm25.k_values)\n",
    "ndcg, _map, recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.08832266181707382, 0.9469413757324219, 0.5297587, 120.60852)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_maxmin(results):\n",
    "    max_score = -1\n",
    "    min_score = 999999\n",
    "    for q_id, q in results.items():\n",
    "        for doc_id, score in q.items():\n",
    "            max_score = max(score, max_score)\n",
    "            min_score = min(score, min_score)\n",
    "\n",
    "    return min_score, max_score\n",
    "\n",
    "# Get range to normalize both\n",
    "min_distilbert_score, max_distilbert_score = get_maxmin(results)\n",
    "min_bm25_score, max_bm25_score = get_maxmin(results_bm25)\n",
    "\n",
    "min_distilbert_score, max_distilbert_score, min_bm25_score, max_bm25_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "def normalize_results(results, min_score, max_score):\n",
    "    for q_id, q in results.items():\n",
    "        for doc_id, score in q.items():\n",
    "            results[q_id][doc_id] = (score-min_score)/(max_score-min_score)\n",
    "\n",
    "    return results\n",
    "\n",
    "results = normalize_results(results, min_distilbert_score, max_distilbert_score)\n",
    "results_bm25 = normalize_results(results_bm25, min_bm25_score, max_bm25_score)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_score(x,y):\n",
    "    mu = 0.5\n",
    "    return mu*x + (1-mu)*y\n",
    "\n",
    "combined_result = {}\n",
    "\n",
    "for q_id_1, q_1 in results.items():\n",
    "        combined_result[q_id_1] = {}\n",
    "        for doc_id_1, score_1 in q_1.items():\n",
    "            \n",
    "            score_2 = 0\n",
    "            if results_bm25[q_id_1].get(doc_id_1,None)!=None:\n",
    "                score_2 = results_bm25[q_id_1][doc_id_1]\n",
    "                del results_bm25[q_id_1][doc_id_1] # So that same query-doc pair is not added to combined result twice\n",
    "            \n",
    "            combined_score = ensemble_score(score_1, score_2)\n",
    "            combined_result[q_id_1][doc_id_1] = combined_score\n",
    "\n",
    "\n",
    "# Now add remaining bm25 results in combined dict\n",
    "for q_id_2, q_2 in results_bm25.items():\n",
    "    for doc_id_2, score_2 in q_2.items():\n",
    "         score_1 = 0\n",
    "         combined_score = ensemble_score(score_1, score_2)\n",
    "         combined_result[q_id_1][doc_id_1] = combined_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-02 21:18:07 - For evaluation, we ignore identical query and document ids (default), please explicitly set ``ignore_identical_ids=False`` to ignore this.\n",
      "2024-06-02 21:18:07 - \n",
      "\n",
      "2024-06-02 21:18:07 - NDCG@1: 0.6167\n",
      "2024-06-02 21:18:07 - NDCG@3: 0.6937\n",
      "2024-06-02 21:18:07 - NDCG@5: 0.7175\n",
      "2024-06-02 21:18:07 - NDCG@10: 0.7391\n",
      "2024-06-02 21:18:07 - NDCG@100: 0.7560\n",
      "2024-06-02 21:18:07 - NDCG@1000: 0.7605\n",
      "2024-06-02 21:18:07 - \n",
      "\n",
      "2024-06-02 21:18:07 - MAP@1: 0.5958\n",
      "2024-06-02 21:18:07 - MAP@3: 0.6672\n",
      "2024-06-02 21:18:07 - MAP@5: 0.6823\n",
      "2024-06-02 21:18:07 - MAP@10: 0.6932\n",
      "2024-06-02 21:18:07 - MAP@100: 0.6973\n",
      "2024-06-02 21:18:07 - MAP@1000: 0.6975\n",
      "2024-06-02 21:18:07 - \n",
      "\n",
      "2024-06-02 21:18:07 - Recall@1: 0.5958\n",
      "2024-06-02 21:18:07 - Recall@3: 0.7508\n",
      "2024-06-02 21:18:07 - Recall@5: 0.8084\n",
      "2024-06-02 21:18:07 - Recall@10: 0.8681\n",
      "2024-06-02 21:18:07 - Recall@100: 0.9430\n",
      "2024-06-02 21:18:07 - Recall@1000: 0.9767\n",
      "2024-06-02 21:18:07 - \n",
      "\n",
      "2024-06-02 21:18:07 - P@1: 0.6167\n",
      "2024-06-02 21:18:07 - P@3: 0.2689\n",
      "2024-06-02 21:18:07 - P@5: 0.1760\n",
      "2024-06-02 21:18:07 - P@10: 0.0967\n",
      "2024-06-02 21:18:07 - P@100: 0.0106\n",
      "2024-06-02 21:18:07 - P@1000: 0.0011\n"
     ]
    }
   ],
   "source": [
    "ndcg, _map, recall, precision = retriever_bm25.evaluate(qrels, combined_result, retriever_bm25.k_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
