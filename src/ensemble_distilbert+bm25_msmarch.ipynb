{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is done in the `distilbert+bm25_msmarco.ipynb` notebook\n",
    "\n",
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 512\n",
    "model_name = \"distilbert-base-uncased\" \n",
    "dataset = \"msmarco_tiny\"\n",
    "\n",
    "dataset_path = \"../beir/datasets/msmarco_tiny/\"\n",
    "corpus_file = \"tiny_collection.json\"\n",
    "queries_file = \"topics.dl20.txt\"\n",
    "qrels_test_file = \"qrels.dl20-passage.txt\"\n",
    "training_set = \"msmarco_triples.train.tiny.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/addullah/miniconda3/envs/myenv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/addullah/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sentence_transformers import losses, models, SentenceTransformer\n",
    "from beir import util, LoggingHandler\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.search.lexical import BM25Search as BM25\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "from beir.retrieval.train import TrainRetriever\n",
    "import pathlib, os, tqdm\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading test set\n",
    "# corpus, queries, qrels = GenericDataLoader(data_path).load(split=\"test\")\n",
    "\n",
    "import collections\n",
    "import pytrec_eval\n",
    "import json\n",
    "\n",
    "def load_queries(path):\n",
    "    \"\"\"Returns a dictionary whose keys are query ids and values are query texts.\"\"\"\n",
    "    queries = {}\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            query_id, query_text = line.strip().split('\\t')\n",
    "            queries[query_id] = query_text\n",
    "    return queries\n",
    "\n",
    "\n",
    "def load_qrels(path):\n",
    "    with open(path, 'r') as f_qrel:\n",
    "        qrels = pytrec_eval.parse_qrel(f_qrel)\n",
    "\n",
    "    return qrels\n",
    "\n",
    "\n",
    "def load_corpus_json(path):\n",
    "    with open(path, 'r') as corpus_f:\n",
    "        corpus_json = json.load(corpus_f)\n",
    "    return corpus_json\n",
    "\n",
    "\n",
    "qrels = load_qrels(f\"{dataset_path}{qrels_test_file}\")\n",
    "queries = load_queries(f\"{dataset_path}{queries_file}\")\n",
    "corpus = load_corpus_json(f\"{dataset_path}{corpus_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BM25 scores\n",
    "\n",
    "with open(f\"{dataset_path}{dataset}_bm25_scores.pickle\", 'rb') as f:\n",
    "    results_bm25 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load distilbert scores\n",
    "\n",
    "with open(f\"{dataset_path}{dataset}_distilBertR_scores.pickle\", 'rb') as f: #_distilBertBM_scores\n",
    "    results_dense = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2672949731349945, 0.9520302414894104, 4.215731, 49.542587)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_maxmin(results):\n",
    "    max_score = -1\n",
    "    min_score = 999999\n",
    "    for q_id, q in results.items():\n",
    "        for doc_id, score in q.items():\n",
    "            max_score = max(score, max_score)\n",
    "            min_score = min(score, min_score)\n",
    "\n",
    "    return min_score, max_score\n",
    "\n",
    "# Get range to normalize both\n",
    "min_distilbert_score, max_distilbert_score = get_maxmin(results_dense)\n",
    "min_bm25_score, max_bm25_score = get_maxmin(results_bm25)\n",
    "\n",
    "min_distilbert_score, max_distilbert_score, min_bm25_score, max_bm25_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "def normalize_results(results, min_score, max_score):\n",
    "    for q_id, q in results.items():\n",
    "        for doc_id, score in q.items():\n",
    "            results[q_id][doc_id] = (score-min_score)/(max_score-min_score)\n",
    "\n",
    "    return results\n",
    "\n",
    "results_dense = normalize_results(results_dense, min_distilbert_score, max_distilbert_score)\n",
    "results_bm25 = normalize_results(results_bm25, min_bm25_score, max_bm25_score)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_score(x,y):\n",
    "    mu = 0.5\n",
    "    return mu*x + (1-mu)*y\n",
    "\n",
    "combined_result = {}\n",
    "\n",
    "for q_id_1, q_1 in results_dense.items():\n",
    "        combined_result[q_id_1] = {}\n",
    "        for doc_id_1, score_1 in q_1.items():\n",
    "            \n",
    "            score_2 = 0\n",
    "            if results_bm25[q_id_1].get(doc_id_1,None)!=None:\n",
    "                score_2 = results_bm25[q_id_1][doc_id_1]\n",
    "                del results_bm25[q_id_1][doc_id_1] # So that same query-doc pair is not added to combined result twice\n",
    "            \n",
    "            combined_score = ensemble_score(score_1, score_2)\n",
    "            combined_result[q_id_1][doc_id_1] = combined_score\n",
    "\n",
    "\n",
    "# Now add remaining bm25 results in combined dict\n",
    "for q_id_2, q_2 in results_bm25.items():\n",
    "    for doc_id_2, score_2 in q_2.items():\n",
    "         score_1 = 0\n",
    "         combined_score = ensemble_score(score_1, score_2)\n",
    "         combined_result[q_id_1][doc_id_1] = combined_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-02 20:56:49 - Activating Elasticsearch....\n",
      "2024-06-02 20:56:49 - Elastic Search Credentials: {'hostname': 'localhost', 'index_name': 'msmarco_tiny', 'keys': {'title': 'title', 'body': 'txt'}, 'timeout': 100, 'retry_on_timeout': True, 'maxsize': 24, 'number_of_shards': 'default', 'language': 'english'}\n",
      "2024-06-02 20:56:49 - Deleting previous Elasticsearch-Index named - msmarco_tiny\n",
      "2024-06-02 20:56:49 - Unable to create Index in Elastic Search. Reason: ConnectionError(('Connection aborted.', BadStatusLine('每\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x7f \\x01dentity\\r\\n'))) caused by: ProtocolError(('Connection aborted.', BadStatusLine('每\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x7f \\x01dentity\\r\\n')))\n",
      "2024-06-02 20:56:51 - Creating fresh Elasticsearch-Index named - msmarco_tiny\n",
      "2024-06-02 20:56:51 - Unable to create Index in Elastic Search. Reason: ConnectionError(('Connection aborted.', BadStatusLine('每\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x7fe\\x00t-Length: 117\\r\\n'))) caused by: ProtocolError(('Connection aborted.', BadStatusLine('每\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x7fe\\x00t-Length: 117\\r\\n')))\n"
     ]
    }
   ],
   "source": [
    "from beir.retrieval.search.lexical import BM25Search as BM25\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "\n",
    "## elasticsearch settings\n",
    "hostname = \"localhost\" #localhost\n",
    "index_name = dataset # scifact\n",
    "initialize = True # True - Delete existing index and re-index all documents from scratch \n",
    "\n",
    "model_bm25 = BM25(index_name=index_name, hostname=hostname, initialize=initialize)\n",
    "retriever_bm25 = EvaluateRetrieval(model_bm25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-02 20:56:52 - For evaluation, we ignore identical query and document ids (default), please explicitly set ``ignore_identical_ids=False`` to ignore this.\n",
      "2024-06-02 20:56:52 - \n",
      "\n",
      "2024-06-02 20:56:52 - NDCG@1: 0.6697\n",
      "2024-06-02 20:56:52 - NDCG@3: 0.6383\n",
      "2024-06-02 20:56:52 - NDCG@5: 0.6230\n",
      "2024-06-02 20:56:52 - NDCG@10: 0.5963\n",
      "2024-06-02 20:56:52 - NDCG@100: 0.6119\n",
      "2024-06-02 20:56:52 - NDCG@1000: 0.6717\n",
      "2024-06-02 20:56:52 - \n",
      "\n",
      "2024-06-02 20:56:52 - MAP@1: 0.0308\n",
      "2024-06-02 20:56:52 - MAP@3: 0.0784\n",
      "2024-06-02 20:56:52 - MAP@5: 0.1075\n",
      "2024-06-02 20:56:52 - MAP@10: 0.1647\n",
      "2024-06-02 20:56:52 - MAP@100: 0.4013\n",
      "2024-06-02 20:56:52 - MAP@1000: 0.4484\n",
      "2024-06-02 20:56:52 - \n",
      "\n",
      "2024-06-02 20:56:52 - Recall@1: 0.0308\n",
      "2024-06-02 20:56:52 - Recall@3: 0.0840\n",
      "2024-06-02 20:56:52 - Recall@5: 0.1170\n",
      "2024-06-02 20:56:52 - Recall@10: 0.1900\n",
      "2024-06-02 20:56:52 - Recall@100: 0.6144\n",
      "2024-06-02 20:56:52 - Recall@1000: 0.7627\n",
      "2024-06-02 20:56:52 - \n",
      "\n",
      "2024-06-02 20:56:52 - P@1: 0.7963\n",
      "2024-06-02 20:56:52 - P@3: 0.7716\n",
      "2024-06-02 20:56:52 - P@5: 0.7370\n",
      "2024-06-02 20:56:52 - P@10: 0.6630\n",
      "2024-06-02 20:56:52 - P@100: 0.3272\n",
      "2024-06-02 20:56:52 - P@1000: 0.0460\n"
     ]
    }
   ],
   "source": [
    "ndcg, _map, recall, precision = retriever_bm25.evaluate(qrels, combined_result, retriever_bm25.k_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
