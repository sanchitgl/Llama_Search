{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/addullah/miniconda3/envs/myenv/lib/python3.12/site-packages/beir/util.py:2: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from beir import util, LoggingHandler\n",
    "from beir.datasets.data_loader import GenericDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"6\"\n",
    "tokenizer_max_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"msmarco_tiny\"\n",
    "\n",
    "dataset_path = \"../beir/datasets/msmarco_tiny/\"\n",
    "corpus_file = \"tiny_collection.json\"\n",
    "queries_file = \"topics.dl20.txt\"\n",
    "qrels_test_file = \"qrels.dl20-passage.txt\"\n",
    "training_set = \"msmarco_triples.train.tiny.tsv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus into memory ...\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import pytrec_eval\n",
    "import json\n",
    "\n",
    "def load_queries(path):\n",
    "    \"\"\"Returns a dictionary whose keys are query ids and values are query texts.\"\"\"\n",
    "    queries = {}\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            query_id, query_text = line.strip().split('\\t')\n",
    "            queries[query_id] = query_text\n",
    "    return queries\n",
    "\n",
    "\n",
    "def load_corpus_tsv(path):\n",
    "    \"\"\"Returns a dictionary whose keys are passage ids and values are passage texts.\"\"\"\n",
    "    corpus = {}\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            passage_id, passage_text = line.strip().split('\\t')\n",
    "            corpus[passage_id] = {'text': passage_text}\n",
    "    return corpus\n",
    "\n",
    "def load_corpus_json(path):\n",
    "    with open(path, 'r') as corpus_f:\n",
    "        corpus_json = json.load(corpus_f)\n",
    "    return corpus_json\n",
    "\n",
    "\n",
    "def load_qrels(path):\n",
    "    with open(path, 'r') as f_qrel:\n",
    "        qrels = pytrec_eval.parse_qrel(f_qrel)\n",
    "\n",
    "    return qrels\n",
    "\n",
    "\n",
    "def load_triplets(path):\n",
    "    triplets = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            query, positive_passage, negative_passage = line.strip().split('\\t')\n",
    "            triplets.append([query, positive_passage, negative_passage])\n",
    "    return triplets\n",
    "\n",
    "# Don't need to load triplet for training bm25\n",
    "# triplets = load_triplets('msmarco_triples.train.tiny.tsv')\n",
    "\n",
    "qrels = load_qrels(f\"{dataset_path}{qrels_test_file}\")\n",
    "queries = load_queries(f\"{dataset_path}{queries_file}\")\n",
    "print(\"Loading corpus into memory ...\")\n",
    "corpus = load_corpus_json(f\"{dataset_path}{corpus_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510585"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/addullah/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.70s/it]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def get_model(peft_model_name):\n",
    "    config = PeftConfig.from_pretrained(peft_model_name)\n",
    "    base_model = AutoModel.from_pretrained(config.base_model_name_or_path)\n",
    "    model = PeftModel.from_pretrained(base_model, peft_model_name)\n",
    "    model = model.merge_and_unload()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "model = get_model('castorini/repllama-v1-7b-lora-passage')\n",
    "\n",
    "model = model.to(device) # Moving model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['qid', 'text'],\n",
       "    num_rows: 300\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from datasets import Dataset\n",
    "# import pandas as pd\n",
    "\n",
    "# queries = pd.DataFrame({'qid': queries.keys(), 'text': queries.values()})\n",
    "# # queries.head()\n",
    "# query_dataset = Dataset.from_pandas(queries)\n",
    "# query_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "#     model.resize_token_embeddings(len(tokenizer)) # Update models token embedding size so it knows about new token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0002,  0.0066,  0.0047,  ..., -0.0012, -0.0056, -0.0186],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_txt = [query_dataset[0]['text'], query_dataset[1]['text']]\n",
    "\n",
    "# tokenized_inputs = tokenizer(input_txt, return_tensors='pt', padding=True, truncation=True, max_length=tokenizer_max_len)\n",
    "# tokenized_inputs.to(device)\n",
    "# with torch.no_grad():\n",
    "#     # compute query embedding\n",
    "#     outputs = model(**tokenized_inputs)\n",
    "#     embedding = outputs.last_hidden_state[0][-1]\n",
    "#     embedding = torch.nn.functional.normalize(embedding, p=2, dim=0)\n",
    "\n",
    "# embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can't batch tokenize because we need embeddings of last token and last token of different models maybe different\n",
    "\n",
    "def get_embed_dataset(input_lst):\n",
    "\n",
    "    input_txt = [f'{input}</s>' for input in input_lst['text']]\n",
    "\n",
    "    tokenized_inputs = tokenizer(input_txt, return_tensors='pt', padding=\"max_length\", truncation=True, max_length=tokenizer_max_len)\n",
    "    tokenized_inputs = tokenized_inputs.to(device)\n",
    "    with torch.no_grad():\n",
    "        # compute query embedding\n",
    "        outputs = model(**tokenized_inputs)\n",
    "        embedding = outputs.last_hidden_state[:,-1,:]   #outputs.last_hidden_state[0][-1] # Get embedding of last token i.e. <s>\n",
    "        embedding = torch.nn.functional.normalize(embedding, p=2, dim=0)\n",
    "    return embedding\n",
    "\n",
    "def get_embed(input):\n",
    "\n",
    "    tokenized_inputs = tokenizer(f'{input}</s>', return_tensors='pt')\n",
    "    tokenized_inputs = tokenized_inputs.to(device)\n",
    "    with torch.no_grad():\n",
    "        # compute query embedding\n",
    "        outputs = model(**tokenized_inputs)\n",
    "        embedding = outputs.last_hidden_state[0][-1] #outputs.last_hidden_state[:,-1,:]  # Get embedding of last token i.e. </s>\n",
    "        embedding = torch.nn.functional.normalize(embedding, p=2, dim=0)\n",
    "    return embedding\n",
    "\n",
    "# query_dataset = query_dataset.map(get_embed, batched=True, batch_size=8)\n",
    "# query_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding queries ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:04<00:00, 11.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding passages ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2184/510585 [03:44<14:31:54,  9.72it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoding passages ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k,q \u001b[38;5;129;01min\u001b[39;00m tqdm(corpus\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[0;32m---> 13\u001b[0m     doc_embed \u001b[38;5;241m=\u001b[39m \u001b[43mget_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     doc_embeddings[k] \u001b[38;5;241m=\u001b[39m doc_embed\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mqueries_llamaEmbed.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m, in \u001b[0;36mget_embed\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m     19\u001b[0m tokenized_inputs \u001b[38;5;241m=\u001b[39m tokenized_inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# compute query embedding\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenized_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m#outputs.last_hidden_state[:,-1,:]  # Get embedding of last token i.e. </s>\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mnormalize(embedding, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:968\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    957\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    958\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    959\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    965\u001b[0m         cache_position,\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    978\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:713\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    710\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 713\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    722\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:616\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    613\u001b[0m bsz, q_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    615\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\n\u001b[0;32m--> 616\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n\u001b[1;32m    619\u001b[0m query_states \u001b[38;5;241m=\u001b[39m query_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1534\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1531\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1532\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1535\u001b[0m     forward_call \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward)\n\u001b[1;32m   1536\u001b[0m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# query_embeddings = {}\n",
    "# doc_embeddings = {}\n",
    "\n",
    "# print(\"Encoding queries ...\")\n",
    "# for k,q in tqdm(queries.items()):\n",
    "#     query_embed = get_embed(q)\n",
    "#     query_embeddings[k] = query_embed\n",
    "\n",
    "# print(\"Encoding passages ...\")\n",
    "# for k,q in tqdm(corpus.items()):\n",
    "#     doc_embed = get_embed(q['text'])\n",
    "#     doc_embeddings[k] = doc_embed\n",
    "\n",
    "# with open(f\"{dataset_path}queries_llamaEmbed.json\", 'w') as f:\n",
    "#     json.dump(query_embeddings, f)\n",
    "\n",
    "# with open(f\"{dataset_path}tiny_collection_llamaEmbed.json\", 'w') as f:\n",
    "#     json.dump(doc_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{dataset_path}tiny_collection_llamaEmbed.pickle\", 'rb') as f:\n",
    "    doc_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{dataset_path}queries_llamaEmbed.pickle\", 'rb') as f:\n",
    "    query_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [11:47<00:00, 13.11s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "results = {}\n",
    "for q_id, q_embed in tqdm(query_embeddings.items()):\n",
    "    results[q_id] = {}\n",
    "    for d_id, d_embed in doc_embeddings.items():\n",
    "        # compute similarity score\n",
    "        score = torch.dot(q_embed, d_embed)\n",
    "        results[q_id][d_id] = score.item() #.item() to get value out of tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nDCG@10: 0.7599\n"
     ]
    }
   ],
   "source": [
    "import pytrec_eval\n",
    "\n",
    "metric = 'ndcg_cut_10'\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrels, {metric})\n",
    "results_metric = evaluator.evaluate(results)\n",
    "print(f'nDCG@10: {sum(item[metric] for item in results_metric.values()) / len(results_metric):0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@10: 0.7110\n"
     ]
    }
   ],
   "source": [
    "metric = 'map_cut_10'\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrels, {metric})\n",
    "results_metric = evaluator.evaluate(results)\n",
    "print(f'MAP@10: {sum(item[metric] for item in results_metric.values()) / len(results_metric):0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unable to create Index in Elastic Search. Reason: ConnectionError(<urllib3.connection.HTTPConnection object at 0x7f19a3f09970>: Failed to establish a new connection: [Errno 111] Connection refused) caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x7f19a3f09970>: Failed to establish a new connection: [Errno 111] Connection refused)\n",
      "ERROR:root:Unable to create Index in Elastic Search. Reason: ConnectionError(<urllib3.connection.HTTPConnection object at 0x7f19a3f0a180>: Failed to establish a new connection: [Errno 111] Connection refused) caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x7f19a3f0a180>: Failed to establish a new connection: [Errno 111] Connection refused)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'NDCG@1': 0.78395,\n",
       "  'NDCG@3': 0.74971,\n",
       "  'NDCG@5': 0.72805,\n",
       "  'NDCG@10': 0.70889,\n",
       "  'NDCG@100': 0.71979,\n",
       "  'NDCG@1000': 0.80971},\n",
       " {'MAP@1': 0.03959,\n",
       "  'MAP@3': 0.08449,\n",
       "  'MAP@5': 0.12773,\n",
       "  'MAP@10': 0.21312,\n",
       "  'MAP@100': 0.53599,\n",
       "  'MAP@1000': 0.60928},\n",
       " {'Recall@1': 0.03959,\n",
       "  'Recall@3': 0.08614,\n",
       "  'Recall@5': 0.13913,\n",
       "  'Recall@10': 0.24466,\n",
       "  'Recall@100': 0.71407,\n",
       "  'Recall@1000': 0.9321},\n",
       " {'P@1': 0.88889,\n",
       "  'P@3': 0.83951,\n",
       "  'P@5': 0.8,\n",
       "  'P@10': 0.76296,\n",
       "  'P@100': 0.38333,\n",
       "  'P@1000': 0.06098})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from beir.retrieval.search.lexical import BM25Search as BM25\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "\n",
    "hostname = \"localhost\"\n",
    "index_name = \"scifact\"\n",
    "initialize = True\n",
    "\n",
    "model = BM25(index_name=index_name, hostname=hostname, initialize=initialize)\n",
    "retriever = EvaluateRetrieval(model)\n",
    "# model doesn't do anything and results for ndcg and map are same as pytrec_eval\n",
    "\n",
    "retriever.evaluate(qrels, results, retriever.k_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'recall_10': 0.0},\n",
       " '3': {'recall_10': 1.0},\n",
       " '5': {'recall_10': 1.0},\n",
       " '13': {'recall_10': 0.0},\n",
       " '36': {'recall_10': 0.5},\n",
       " '42': {'recall_10': 1.0},\n",
       " '48': {'recall_10': 1.0},\n",
       " '49': {'recall_10': 1.0},\n",
       " '50': {'recall_10': 1.0},\n",
       " '51': {'recall_10': 1.0},\n",
       " '53': {'recall_10': 1.0},\n",
       " '54': {'recall_10': 1.0},\n",
       " '56': {'recall_10': 1.0},\n",
       " '57': {'recall_10': 1.0},\n",
       " '70': {'recall_10': 1.0},\n",
       " '72': {'recall_10': 1.0},\n",
       " '75': {'recall_10': 1.0},\n",
       " '94': {'recall_10': 1.0},\n",
       " '99': {'recall_10': 1.0},\n",
       " '100': {'recall_10': 1.0},\n",
       " '113': {'recall_10': 1.0},\n",
       " '115': {'recall_10': 1.0},\n",
       " '118': {'recall_10': 1.0},\n",
       " '124': {'recall_10': 1.0},\n",
       " '127': {'recall_10': 1.0},\n",
       " '128': {'recall_10': 0.0},\n",
       " '129': {'recall_10': 1.0},\n",
       " '130': {'recall_10': 1.0},\n",
       " '132': {'recall_10': 0.0},\n",
       " '133': {'recall_10': 0.8},\n",
       " '137': {'recall_10': 1.0},\n",
       " '141': {'recall_10': 1.0},\n",
       " '142': {'recall_10': 1.0},\n",
       " '143': {'recall_10': 1.0},\n",
       " '146': {'recall_10': 1.0},\n",
       " '148': {'recall_10': 1.0},\n",
       " '163': {'recall_10': 1.0},\n",
       " '171': {'recall_10': 1.0},\n",
       " '179': {'recall_10': 1.0},\n",
       " '180': {'recall_10': 1.0},\n",
       " '183': {'recall_10': 1.0},\n",
       " '185': {'recall_10': 1.0},\n",
       " '198': {'recall_10': 1.0},\n",
       " '208': {'recall_10': 1.0},\n",
       " '212': {'recall_10': 1.0},\n",
       " '213': {'recall_10': 1.0},\n",
       " '216': {'recall_10': 1.0},\n",
       " '217': {'recall_10': 1.0},\n",
       " '218': {'recall_10': 1.0},\n",
       " '219': {'recall_10': 1.0},\n",
       " '230': {'recall_10': 1.0},\n",
       " '232': {'recall_10': 1.0},\n",
       " '233': {'recall_10': 1.0},\n",
       " '236': {'recall_10': 1.0},\n",
       " '237': {'recall_10': 1.0},\n",
       " '238': {'recall_10': 1.0},\n",
       " '239': {'recall_10': 1.0},\n",
       " '248': {'recall_10': 1.0},\n",
       " '249': {'recall_10': 1.0},\n",
       " '261': {'recall_10': 1.0},\n",
       " '268': {'recall_10': 1.0},\n",
       " '269': {'recall_10': 1.0},\n",
       " '274': {'recall_10': 1.0},\n",
       " '275': {'recall_10': 1.0},\n",
       " '279': {'recall_10': 1.0},\n",
       " '294': {'recall_10': 1.0},\n",
       " '295': {'recall_10': 1.0},\n",
       " '298': {'recall_10': 1.0},\n",
       " '300': {'recall_10': 1.0},\n",
       " '303': {'recall_10': 0.0},\n",
       " '312': {'recall_10': 1.0},\n",
       " '314': {'recall_10': 1.0},\n",
       " '324': {'recall_10': 1.0},\n",
       " '327': {'recall_10': 1.0},\n",
       " '338': {'recall_10': 1.0},\n",
       " '343': {'recall_10': 1.0},\n",
       " '350': {'recall_10': 1.0},\n",
       " '354': {'recall_10': 1.0},\n",
       " '362': {'recall_10': 1.0},\n",
       " '380': {'recall_10': 1.0},\n",
       " '384': {'recall_10': 1.0},\n",
       " '385': {'recall_10': 1.0},\n",
       " '386': {'recall_10': 1.0},\n",
       " '388': {'recall_10': 1.0},\n",
       " '399': {'recall_10': 1.0},\n",
       " '410': {'recall_10': 1.0},\n",
       " '411': {'recall_10': 1.0},\n",
       " '415': {'recall_10': 1.0},\n",
       " '421': {'recall_10': 0.0},\n",
       " '431': {'recall_10': 0.0},\n",
       " '436': {'recall_10': 1.0},\n",
       " '437': {'recall_10': 0.0},\n",
       " '439': {'recall_10': 1.0},\n",
       " '440': {'recall_10': 1.0},\n",
       " '443': {'recall_10': 1.0},\n",
       " '452': {'recall_10': 0.5},\n",
       " '475': {'recall_10': 0.0},\n",
       " '478': {'recall_10': 1.0},\n",
       " '491': {'recall_10': 1.0},\n",
       " '501': {'recall_10': 1.0},\n",
       " '502': {'recall_10': 0.0},\n",
       " '507': {'recall_10': 1.0},\n",
       " '508': {'recall_10': 1.0},\n",
       " '513': {'recall_10': 1.0},\n",
       " '514': {'recall_10': 1.0},\n",
       " '516': {'recall_10': 1.0},\n",
       " '517': {'recall_10': 1.0},\n",
       " '521': {'recall_10': 1.0},\n",
       " '525': {'recall_10': 1.0},\n",
       " '527': {'recall_10': 1.0},\n",
       " '528': {'recall_10': 1.0},\n",
       " '532': {'recall_10': 1.0},\n",
       " '533': {'recall_10': 1.0},\n",
       " '535': {'recall_10': 1.0},\n",
       " '536': {'recall_10': 1.0},\n",
       " '539': {'recall_10': 1.0},\n",
       " '540': {'recall_10': 0.5},\n",
       " '544': {'recall_10': 1.0},\n",
       " '549': {'recall_10': 1.0},\n",
       " '551': {'recall_10': 1.0},\n",
       " '552': {'recall_10': 1.0},\n",
       " '554': {'recall_10': 1.0},\n",
       " '560': {'recall_10': 0.0},\n",
       " '569': {'recall_10': 1.0},\n",
       " '575': {'recall_10': 1.0},\n",
       " '577': {'recall_10': 1.0},\n",
       " '578': {'recall_10': 1.0},\n",
       " '587': {'recall_10': 1.0},\n",
       " '589': {'recall_10': 1.0},\n",
       " '593': {'recall_10': 1.0},\n",
       " '597': {'recall_10': 1.0},\n",
       " '598': {'recall_10': 1.0},\n",
       " '613': {'recall_10': 1.0},\n",
       " '619': {'recall_10': 1.0},\n",
       " '623': {'recall_10': 1.0},\n",
       " '628': {'recall_10': 1.0},\n",
       " '636': {'recall_10': 1.0},\n",
       " '637': {'recall_10': 1.0},\n",
       " '641': {'recall_10': 1.0},\n",
       " '644': {'recall_10': 1.0},\n",
       " '649': {'recall_10': 1.0},\n",
       " '659': {'recall_10': 1.0},\n",
       " '660': {'recall_10': 1.0},\n",
       " '674': {'recall_10': 1.0},\n",
       " '684': {'recall_10': 1.0},\n",
       " '690': {'recall_10': 1.0},\n",
       " '691': {'recall_10': 1.0},\n",
       " '692': {'recall_10': 1.0},\n",
       " '693': {'recall_10': 1.0},\n",
       " '700': {'recall_10': 1.0},\n",
       " '702': {'recall_10': 1.0},\n",
       " '715': {'recall_10': 0.0},\n",
       " '716': {'recall_10': 0.0},\n",
       " '718': {'recall_10': 1.0},\n",
       " '721': {'recall_10': 1.0},\n",
       " '723': {'recall_10': 1.0},\n",
       " '727': {'recall_10': 1.0},\n",
       " '728': {'recall_10': 1.0},\n",
       " '729': {'recall_10': 1.0},\n",
       " '742': {'recall_10': 1.0},\n",
       " '743': {'recall_10': 1.0},\n",
       " '744': {'recall_10': 1.0},\n",
       " '756': {'recall_10': 1.0},\n",
       " '759': {'recall_10': 1.0},\n",
       " '768': {'recall_10': 1.0},\n",
       " '770': {'recall_10': 1.0},\n",
       " '775': {'recall_10': 0.0},\n",
       " '781': {'recall_10': 1.0},\n",
       " '783': {'recall_10': 1.0},\n",
       " '784': {'recall_10': 1.0},\n",
       " '785': {'recall_10': 1.0},\n",
       " '793': {'recall_10': 1.0},\n",
       " '800': {'recall_10': 1.0},\n",
       " '805': {'recall_10': 1.0},\n",
       " '808': {'recall_10': 1.0},\n",
       " '811': {'recall_10': 1.0},\n",
       " '814': {'recall_10': 1.0},\n",
       " '820': {'recall_10': 1.0},\n",
       " '821': {'recall_10': 1.0},\n",
       " '823': {'recall_10': 1.0},\n",
       " '830': {'recall_10': 1.0},\n",
       " '831': {'recall_10': 0.0},\n",
       " '832': {'recall_10': 1.0},\n",
       " '834': {'recall_10': 0.0},\n",
       " '837': {'recall_10': 1.0},\n",
       " '839': {'recall_10': 1.0},\n",
       " '845': {'recall_10': 1.0},\n",
       " '847': {'recall_10': 1.0},\n",
       " '852': {'recall_10': 1.0},\n",
       " '859': {'recall_10': 1.0},\n",
       " '870': {'recall_10': 0.0},\n",
       " '873': {'recall_10': 0.8},\n",
       " '879': {'recall_10': 1.0},\n",
       " '880': {'recall_10': 1.0},\n",
       " '882': {'recall_10': 1.0},\n",
       " '887': {'recall_10': 0.0},\n",
       " '903': {'recall_10': 1.0},\n",
       " '904': {'recall_10': 1.0},\n",
       " '907': {'recall_10': 1.0},\n",
       " '911': {'recall_10': 1.0},\n",
       " '913': {'recall_10': 1.0},\n",
       " '914': {'recall_10': 1.0},\n",
       " '921': {'recall_10': 1.0},\n",
       " '922': {'recall_10': 1.0},\n",
       " '936': {'recall_10': 1.0},\n",
       " '956': {'recall_10': 1.0},\n",
       " '957': {'recall_10': 1.0},\n",
       " '960': {'recall_10': 1.0},\n",
       " '967': {'recall_10': 1.0},\n",
       " '971': {'recall_10': 1.0},\n",
       " '975': {'recall_10': 0.0},\n",
       " '982': {'recall_10': 1.0},\n",
       " '985': {'recall_10': 1.0},\n",
       " '993': {'recall_10': 1.0},\n",
       " '1012': {'recall_10': 1.0},\n",
       " '1014': {'recall_10': 1.0},\n",
       " '1019': {'recall_10': 1.0},\n",
       " '1020': {'recall_10': 1.0},\n",
       " '1021': {'recall_10': 1.0},\n",
       " '1024': {'recall_10': 1.0},\n",
       " '1029': {'recall_10': 1.0},\n",
       " '1041': {'recall_10': 0.5},\n",
       " '1049': {'recall_10': 1.0},\n",
       " '1062': {'recall_10': 1.0},\n",
       " '1086': {'recall_10': 1.0},\n",
       " '1088': {'recall_10': 1.0},\n",
       " '1089': {'recall_10': 1.0},\n",
       " '1099': {'recall_10': 1.0},\n",
       " '1100': {'recall_10': 0.0},\n",
       " '1104': {'recall_10': 1.0},\n",
       " '1107': {'recall_10': 1.0},\n",
       " '1110': {'recall_10': 0.0},\n",
       " '1121': {'recall_10': 1.0},\n",
       " '1130': {'recall_10': 1.0},\n",
       " '1132': {'recall_10': 1.0},\n",
       " '1137': {'recall_10': 1.0},\n",
       " '1140': {'recall_10': 1.0},\n",
       " '1144': {'recall_10': 1.0},\n",
       " '1146': {'recall_10': 1.0},\n",
       " '1150': {'recall_10': 1.0},\n",
       " '1163': {'recall_10': 1.0},\n",
       " '1175': {'recall_10': 0.0},\n",
       " '1179': {'recall_10': 1.0},\n",
       " '1180': {'recall_10': 1.0},\n",
       " '1185': {'recall_10': 1.0},\n",
       " '1187': {'recall_10': 1.0},\n",
       " '1191': {'recall_10': 1.0},\n",
       " '1194': {'recall_10': 1.0},\n",
       " '1196': {'recall_10': 1.0},\n",
       " '1197': {'recall_10': 1.0},\n",
       " '1199': {'recall_10': 0.0},\n",
       " '1200': {'recall_10': 1.0},\n",
       " '1202': {'recall_10': 1.0},\n",
       " '1204': {'recall_10': 1.0},\n",
       " '1207': {'recall_10': 1.0},\n",
       " '1213': {'recall_10': 0.0},\n",
       " '1216': {'recall_10': 1.0},\n",
       " '1221': {'recall_10': 1.0},\n",
       " '1225': {'recall_10': 1.0},\n",
       " '1226': {'recall_10': 1.0},\n",
       " '1232': {'recall_10': 1.0},\n",
       " '1241': {'recall_10': 1.0},\n",
       " '1245': {'recall_10': 1.0},\n",
       " '1259': {'recall_10': 1.0},\n",
       " '1262': {'recall_10': 1.0},\n",
       " '1266': {'recall_10': 1.0},\n",
       " '1270': {'recall_10': 1.0},\n",
       " '1271': {'recall_10': 1.0},\n",
       " '1272': {'recall_10': 1.0},\n",
       " '1273': {'recall_10': 1.0},\n",
       " '1274': {'recall_10': 1.0},\n",
       " '1278': {'recall_10': 1.0},\n",
       " '1279': {'recall_10': 1.0},\n",
       " '1280': {'recall_10': 1.0},\n",
       " '1281': {'recall_10': 1.0},\n",
       " '1282': {'recall_10': 1.0},\n",
       " '1290': {'recall_10': 1.0},\n",
       " '1292': {'recall_10': 1.0},\n",
       " '1298': {'recall_10': 1.0},\n",
       " '1303': {'recall_10': 1.0},\n",
       " '1316': {'recall_10': 0.0},\n",
       " '1319': {'recall_10': 1.0},\n",
       " '1320': {'recall_10': 1.0},\n",
       " '1332': {'recall_10': 0.0},\n",
       " '1335': {'recall_10': 1.0},\n",
       " '1336': {'recall_10': 1.0},\n",
       " '1337': {'recall_10': 1.0},\n",
       " '1339': {'recall_10': 1.0},\n",
       " '1344': {'recall_10': 1.0},\n",
       " '1352': {'recall_10': 1.0},\n",
       " '1359': {'recall_10': 1.0},\n",
       " '1362': {'recall_10': 0.0},\n",
       " '1363': {'recall_10': 0.0},\n",
       " '1368': {'recall_10': 1.0},\n",
       " '1370': {'recall_10': 1.0},\n",
       " '1379': {'recall_10': 1.0},\n",
       " '1382': {'recall_10': 1.0},\n",
       " '1385': {'recall_10': 1.0},\n",
       " '1389': {'recall_10': 1.0},\n",
       " '1395': {'recall_10': 1.0}}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = 'recall.10'\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrels, {metric})\n",
    "results_metric = evaluator.evaluate(results)\n",
    "# print(f'recall@10: {sum(item[metric] for item in results_metric.values()) / len(results_metric):0.4f}')\n",
    "results_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'P_10': 0.0},\n",
       " '3': {'P_10': 0.1},\n",
       " '5': {'P_10': 0.1},\n",
       " '13': {'P_10': 0.0},\n",
       " '36': {'P_10': 0.1},\n",
       " '42': {'P_10': 0.1},\n",
       " '48': {'P_10': 0.1},\n",
       " '49': {'P_10': 0.1},\n",
       " '50': {'P_10': 0.1},\n",
       " '51': {'P_10': 0.1},\n",
       " '53': {'P_10': 0.1},\n",
       " '54': {'P_10': 0.1},\n",
       " '56': {'P_10': 0.1},\n",
       " '57': {'P_10': 0.1},\n",
       " '70': {'P_10': 0.2},\n",
       " '72': {'P_10': 0.1},\n",
       " '75': {'P_10': 0.1},\n",
       " '94': {'P_10': 0.1},\n",
       " '99': {'P_10': 0.1},\n",
       " '100': {'P_10': 0.1},\n",
       " '113': {'P_10': 0.1},\n",
       " '115': {'P_10': 0.1},\n",
       " '118': {'P_10': 0.1},\n",
       " '124': {'P_10': 0.1},\n",
       " '127': {'P_10': 0.1},\n",
       " '128': {'P_10': 0.0},\n",
       " '129': {'P_10': 0.1},\n",
       " '130': {'P_10': 0.1},\n",
       " '132': {'P_10': 0.0},\n",
       " '133': {'P_10': 0.4},\n",
       " '137': {'P_10': 0.1},\n",
       " '141': {'P_10': 0.2},\n",
       " '142': {'P_10': 0.1},\n",
       " '143': {'P_10': 0.1},\n",
       " '146': {'P_10': 0.1},\n",
       " '148': {'P_10': 0.1},\n",
       " '163': {'P_10': 0.1},\n",
       " '171': {'P_10': 0.1},\n",
       " '179': {'P_10': 0.4},\n",
       " '180': {'P_10': 0.1},\n",
       " '183': {'P_10': 0.1},\n",
       " '185': {'P_10': 0.1},\n",
       " '198': {'P_10': 0.1},\n",
       " '208': {'P_10': 0.1},\n",
       " '212': {'P_10': 0.1},\n",
       " '213': {'P_10': 0.1},\n",
       " '216': {'P_10': 0.1},\n",
       " '217': {'P_10': 0.1},\n",
       " '218': {'P_10': 0.1},\n",
       " '219': {'P_10': 0.1},\n",
       " '230': {'P_10': 0.1},\n",
       " '232': {'P_10': 0.1},\n",
       " '233': {'P_10': 0.1},\n",
       " '236': {'P_10': 0.1},\n",
       " '237': {'P_10': 0.1},\n",
       " '238': {'P_10': 0.1},\n",
       " '239': {'P_10': 0.1},\n",
       " '248': {'P_10': 0.1},\n",
       " '249': {'P_10': 0.1},\n",
       " '261': {'P_10': 0.2},\n",
       " '268': {'P_10': 0.1},\n",
       " '269': {'P_10': 0.1},\n",
       " '274': {'P_10': 0.1},\n",
       " '275': {'P_10': 0.3},\n",
       " '279': {'P_10': 0.1},\n",
       " '294': {'P_10': 0.1},\n",
       " '295': {'P_10': 0.1},\n",
       " '298': {'P_10': 0.1},\n",
       " '300': {'P_10': 0.1},\n",
       " '303': {'P_10': 0.0},\n",
       " '312': {'P_10': 0.1},\n",
       " '314': {'P_10': 0.1},\n",
       " '324': {'P_10': 0.1},\n",
       " '327': {'P_10': 0.1},\n",
       " '338': {'P_10': 0.1},\n",
       " '343': {'P_10': 0.2},\n",
       " '350': {'P_10': 0.1},\n",
       " '354': {'P_10': 0.1},\n",
       " '362': {'P_10': 0.1},\n",
       " '380': {'P_10': 0.1},\n",
       " '384': {'P_10': 0.1},\n",
       " '385': {'P_10': 0.2},\n",
       " '386': {'P_10': 0.1},\n",
       " '388': {'P_10': 0.1},\n",
       " '399': {'P_10': 0.1},\n",
       " '410': {'P_10': 0.1},\n",
       " '411': {'P_10': 0.1},\n",
       " '415': {'P_10': 0.1},\n",
       " '421': {'P_10': 0.0},\n",
       " '431': {'P_10': 0.0},\n",
       " '436': {'P_10': 0.1},\n",
       " '437': {'P_10': 0.0},\n",
       " '439': {'P_10': 0.1},\n",
       " '440': {'P_10': 0.1},\n",
       " '443': {'P_10': 0.1},\n",
       " '452': {'P_10': 0.1},\n",
       " '475': {'P_10': 0.0},\n",
       " '478': {'P_10': 0.1},\n",
       " '491': {'P_10': 0.1},\n",
       " '501': {'P_10': 0.1},\n",
       " '502': {'P_10': 0.0},\n",
       " '507': {'P_10': 0.1},\n",
       " '508': {'P_10': 0.1},\n",
       " '513': {'P_10': 0.1},\n",
       " '514': {'P_10': 0.1},\n",
       " '516': {'P_10': 0.1},\n",
       " '517': {'P_10': 0.1},\n",
       " '521': {'P_10': 0.1},\n",
       " '525': {'P_10': 0.1},\n",
       " '527': {'P_10': 0.1},\n",
       " '528': {'P_10': 0.1},\n",
       " '532': {'P_10': 0.1},\n",
       " '533': {'P_10': 0.1},\n",
       " '535': {'P_10': 0.1},\n",
       " '536': {'P_10': 0.1},\n",
       " '539': {'P_10': 0.1},\n",
       " '540': {'P_10': 0.1},\n",
       " '544': {'P_10': 0.1},\n",
       " '549': {'P_10': 0.1},\n",
       " '551': {'P_10': 0.1},\n",
       " '552': {'P_10': 0.1},\n",
       " '554': {'P_10': 0.1},\n",
       " '560': {'P_10': 0.0},\n",
       " '569': {'P_10': 0.1},\n",
       " '575': {'P_10': 0.1},\n",
       " '577': {'P_10': 0.1},\n",
       " '578': {'P_10': 0.1},\n",
       " '587': {'P_10': 0.1},\n",
       " '589': {'P_10': 0.1},\n",
       " '593': {'P_10': 0.1},\n",
       " '597': {'P_10': 0.3},\n",
       " '598': {'P_10': 0.1},\n",
       " '613': {'P_10': 0.1},\n",
       " '619': {'P_10': 0.2},\n",
       " '623': {'P_10': 0.1},\n",
       " '628': {'P_10': 0.1},\n",
       " '636': {'P_10': 0.1},\n",
       " '637': {'P_10': 0.1},\n",
       " '641': {'P_10': 0.2},\n",
       " '644': {'P_10': 0.1},\n",
       " '649': {'P_10': 0.1},\n",
       " '659': {'P_10': 0.1},\n",
       " '660': {'P_10': 0.1},\n",
       " '674': {'P_10': 0.1},\n",
       " '684': {'P_10': 0.1},\n",
       " '690': {'P_10': 0.1},\n",
       " '691': {'P_10': 0.1},\n",
       " '692': {'P_10': 0.1},\n",
       " '693': {'P_10': 0.1},\n",
       " '700': {'P_10': 0.1},\n",
       " '702': {'P_10': 0.1},\n",
       " '715': {'P_10': 0.0},\n",
       " '716': {'P_10': 0.0},\n",
       " '718': {'P_10': 0.1},\n",
       " '721': {'P_10': 0.1},\n",
       " '723': {'P_10': 0.1},\n",
       " '727': {'P_10': 0.1},\n",
       " '728': {'P_10': 0.2},\n",
       " '729': {'P_10': 0.1},\n",
       " '742': {'P_10': 0.1},\n",
       " '743': {'P_10': 0.1},\n",
       " '744': {'P_10': 0.1},\n",
       " '756': {'P_10': 0.1},\n",
       " '759': {'P_10': 0.1},\n",
       " '768': {'P_10': 0.1},\n",
       " '770': {'P_10': 0.1},\n",
       " '775': {'P_10': 0.0},\n",
       " '781': {'P_10': 0.1},\n",
       " '783': {'P_10': 0.1},\n",
       " '784': {'P_10': 0.1},\n",
       " '785': {'P_10': 0.1},\n",
       " '793': {'P_10': 0.1},\n",
       " '800': {'P_10': 0.1},\n",
       " '805': {'P_10': 0.1},\n",
       " '808': {'P_10': 0.1},\n",
       " '811': {'P_10': 0.1},\n",
       " '814': {'P_10': 0.1},\n",
       " '820': {'P_10': 0.1},\n",
       " '821': {'P_10': 0.1},\n",
       " '823': {'P_10': 0.1},\n",
       " '830': {'P_10': 0.1},\n",
       " '831': {'P_10': 0.0},\n",
       " '832': {'P_10': 0.1},\n",
       " '834': {'P_10': 0.0},\n",
       " '837': {'P_10': 0.1},\n",
       " '839': {'P_10': 0.1},\n",
       " '845': {'P_10': 0.1},\n",
       " '847': {'P_10': 0.1},\n",
       " '852': {'P_10': 0.1},\n",
       " '859': {'P_10': 0.1},\n",
       " '870': {'P_10': 0.0},\n",
       " '873': {'P_10': 0.4},\n",
       " '879': {'P_10': 0.1},\n",
       " '880': {'P_10': 0.1},\n",
       " '882': {'P_10': 0.1},\n",
       " '887': {'P_10': 0.0},\n",
       " '903': {'P_10': 0.1},\n",
       " '904': {'P_10': 0.1},\n",
       " '907': {'P_10': 0.1},\n",
       " '911': {'P_10': 0.1},\n",
       " '913': {'P_10': 0.1},\n",
       " '914': {'P_10': 0.1},\n",
       " '921': {'P_10': 0.1},\n",
       " '922': {'P_10': 0.1},\n",
       " '936': {'P_10': 0.1},\n",
       " '956': {'P_10': 0.1},\n",
       " '957': {'P_10': 0.1},\n",
       " '960': {'P_10': 0.1},\n",
       " '967': {'P_10': 0.2},\n",
       " '971': {'P_10': 0.4},\n",
       " '975': {'P_10': 0.0},\n",
       " '982': {'P_10': 0.1},\n",
       " '985': {'P_10': 0.1},\n",
       " '993': {'P_10': 0.1},\n",
       " '1012': {'P_10': 0.1},\n",
       " '1014': {'P_10': 0.1},\n",
       " '1019': {'P_10': 0.1},\n",
       " '1020': {'P_10': 0.1},\n",
       " '1021': {'P_10': 0.1},\n",
       " '1024': {'P_10': 0.1},\n",
       " '1029': {'P_10': 0.3},\n",
       " '1041': {'P_10': 0.1},\n",
       " '1049': {'P_10': 0.1},\n",
       " '1062': {'P_10': 0.1},\n",
       " '1086': {'P_10': 0.1},\n",
       " '1088': {'P_10': 0.1},\n",
       " '1089': {'P_10': 0.1},\n",
       " '1099': {'P_10': 0.1},\n",
       " '1100': {'P_10': 0.0},\n",
       " '1104': {'P_10': 0.1},\n",
       " '1107': {'P_10': 0.1},\n",
       " '1110': {'P_10': 0.0},\n",
       " '1121': {'P_10': 0.1},\n",
       " '1130': {'P_10': 0.1},\n",
       " '1132': {'P_10': 0.2},\n",
       " '1137': {'P_10': 0.1},\n",
       " '1140': {'P_10': 0.1},\n",
       " '1144': {'P_10': 0.1},\n",
       " '1146': {'P_10': 0.1},\n",
       " '1150': {'P_10': 0.1},\n",
       " '1163': {'P_10': 0.1},\n",
       " '1175': {'P_10': 0.0},\n",
       " '1179': {'P_10': 0.1},\n",
       " '1180': {'P_10': 0.1},\n",
       " '1185': {'P_10': 0.1},\n",
       " '1187': {'P_10': 0.1},\n",
       " '1191': {'P_10': 0.1},\n",
       " '1194': {'P_10': 0.1},\n",
       " '1196': {'P_10': 0.1},\n",
       " '1197': {'P_10': 0.1},\n",
       " '1199': {'P_10': 0.0},\n",
       " '1200': {'P_10': 0.1},\n",
       " '1202': {'P_10': 0.1},\n",
       " '1204': {'P_10': 0.1},\n",
       " '1207': {'P_10': 0.1},\n",
       " '1213': {'P_10': 0.0},\n",
       " '1216': {'P_10': 0.1},\n",
       " '1221': {'P_10': 0.1},\n",
       " '1225': {'P_10': 0.1},\n",
       " '1226': {'P_10': 0.1},\n",
       " '1232': {'P_10': 0.1},\n",
       " '1241': {'P_10': 0.1},\n",
       " '1245': {'P_10': 0.1},\n",
       " '1259': {'P_10': 0.1},\n",
       " '1262': {'P_10': 0.1},\n",
       " '1266': {'P_10': 0.1},\n",
       " '1270': {'P_10': 0.1},\n",
       " '1271': {'P_10': 0.1},\n",
       " '1272': {'P_10': 0.1},\n",
       " '1273': {'P_10': 0.1},\n",
       " '1274': {'P_10': 0.3},\n",
       " '1278': {'P_10': 0.1},\n",
       " '1279': {'P_10': 0.1},\n",
       " '1280': {'P_10': 0.1},\n",
       " '1281': {'P_10': 0.1},\n",
       " '1282': {'P_10': 0.1},\n",
       " '1290': {'P_10': 0.1},\n",
       " '1292': {'P_10': 0.1},\n",
       " '1298': {'P_10': 0.1},\n",
       " '1303': {'P_10': 0.1},\n",
       " '1316': {'P_10': 0.0},\n",
       " '1319': {'P_10': 0.1},\n",
       " '1320': {'P_10': 0.1},\n",
       " '1332': {'P_10': 0.0},\n",
       " '1335': {'P_10': 0.1},\n",
       " '1336': {'P_10': 0.1},\n",
       " '1337': {'P_10': 0.1},\n",
       " '1339': {'P_10': 0.1},\n",
       " '1344': {'P_10': 0.1},\n",
       " '1352': {'P_10': 0.1},\n",
       " '1359': {'P_10': 0.1},\n",
       " '1362': {'P_10': 0.0},\n",
       " '1363': {'P_10': 0.0},\n",
       " '1368': {'P_10': 0.1},\n",
       " '1370': {'P_10': 0.1},\n",
       " '1379': {'P_10': 0.4},\n",
       " '1382': {'P_10': 0.1},\n",
       " '1385': {'P_10': 0.1},\n",
       " '1389': {'P_10': 0.1},\n",
       " '1395': {'P_10': 0.1}}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = 'P.10'\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrels, {metric})\n",
    "results_metric = evaluator.evaluate(results)\n",
    "# print(f'P@10: {sum(item[metric] for item in results_metric.values()) / len(results_metric):0.4f}')\n",
    "results_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
