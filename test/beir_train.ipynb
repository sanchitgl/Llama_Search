{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sbert training\n",
    "\n",
    "https://github.com/beir-cellar/beir/blob/main/examples/retrieval/training/train_sbert_BM25_hardnegs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ab/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sentence_transformers import losses, models, SentenceTransformer\n",
    "from beir import util, LoggingHandler\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.search.lexical import BM25Search as BM25\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "from beir.retrieval.train import TrainRetriever\n",
    "import pathlib, os, tqdm\n",
    "import logging\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Download nfcorpus.zip dataset and unzip the dataset\n",
    "dataset = \"scifact\"\n",
    "\n",
    "url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{}.zip\".format(dataset)\n",
    "out_dir = os.path.join(os.getcwd(), \"datasets\")\n",
    "data_path = util.download_and_unzip(url, out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-28 20:47:38 - Loading Corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5183/5183 [00:00<00:00, 17911.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-28 20:47:39 - Loaded 5183 TRAIN Documents.\n",
      "2024-05-28 20:47:39 - Doc Example: {'text': 'Alterations of the architecture of cerebral white matter in the developing human brain can affect cortical development and result in functional disabilities. A line scan diffusion-weighted magnetic resonance imaging (MRI) sequence with diffusion tensor analysis was applied to measure the apparent diffusion coefficient, to calculate relative anisotropy, and to delineate three-dimensional fiber architecture in cerebral white matter in preterm (n = 17) and full-term infants (n = 7). To assess effects of prematurity on cerebral white matter development, early gestation preterm infants (n = 10) were studied a second time at term. In the central white matter the mean apparent diffusion coefficient at 28 wk was high, 1.8 microm2/ms, and decreased toward term to 1.2 microm2/ms. In the posterior limb of the internal capsule, the mean apparent diffusion coefficients at both times were similar (1.2 versus 1.1 microm2/ms). Relative anisotropy was higher the closer birth was to term with greater absolute values in the internal capsule than in the central white matter. Preterm infants at term showed higher mean diffusion coefficients in the central white matter (1.4 +/- 0.24 versus 1.15 +/- 0.09 microm2/ms, p = 0.016) and lower relative anisotropy in both areas compared with full-term infants (white matter, 10.9 +/- 0.6 versus 22.9 +/- 3.0%, p = 0.001; internal capsule, 24.0 +/- 4.44 versus 33.1 +/- 0.6% p = 0.006). Nonmyelinated fibers in the corpus callosum were visible by diffusion tensor MRI as early as 28 wk; full-term and preterm infants at term showed marked differences in white matter fiber organization. The data indicate that quantitative assessment of water diffusion by diffusion tensor MRI provides insight into microstructural development in cerebral white matter in living infants.', 'title': 'Microstructural development of human newborn cerebral white matter assessed in vivo by diffusion tensor magnetic resonance imaging.'}\n",
      "2024-05-28 20:47:39 - Loading Queries...\n",
      "2024-05-28 20:47:39 - Loaded 809 TRAIN Queries.\n",
      "2024-05-28 20:47:39 - Query Example: 0-dimensional biomaterials lack inductive properties.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### Provide the data_path where scifact has been downloaded and unzipped\n",
    "corpus, queries, qrels = GenericDataLoader(data_path).load(split=\"train\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-28 20:47:39 - Activating Elasticsearch....\n",
      "2024-05-28 20:47:39 - Elastic Search Credentials: {'hostname': 'localhost', 'index_name': 'scifact', 'keys': {'title': 'title', 'body': 'txt'}, 'timeout': 100, 'retry_on_timeout': True, 'maxsize': 24, 'number_of_shards': 1, 'language': 'english'}\n",
      "2024-05-28 20:47:39 - Deleting previous Elasticsearch-Index named - scifact\n",
      "2024-05-28 20:47:42 - Creating fresh Elasticsearch-Index named - scifact\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### Lexical Retrieval using Bm25 (Elasticsearch) ####\n",
    "#### Provide a hostname (localhost) to connect to ES instance\n",
    "#### Define a new index name or use an already existing one.\n",
    "#### We use default ES settings for retrieval\n",
    "#### https://www.elastic.co/\n",
    "\n",
    "hostname = \"localhost\" #localhost\n",
    "index_name = \"scifact\" # scifact\n",
    "\n",
    "#### Intialize #### \n",
    "# (1) True - Delete existing index and re-index all documents from scratch \n",
    "# (2) False - Load existing index\n",
    "initialize = True # False\n",
    "\n",
    "#### Sharding ####\n",
    "# (1) For datasets with small corpus (datasets ~ < 5k docs) => limit shards = 1 \n",
    "# SciFact is a relatively small dataset! (limit shards to 1)\n",
    "number_of_shards = 1\n",
    "model = BM25(index_name=index_name, hostname=hostname, initialize=initialize, number_of_shards=number_of_shards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5183 [00:00<?, ?docs/s]             \n",
      "Retrieve Hard Negatives using BM25: 100%|██████████| 809/809 [00:11<00:00, 72.77it/s] \n"
     ]
    }
   ],
   "source": [
    "bm25 = EvaluateRetrieval(model)\n",
    "\n",
    "#### Index passages into the index (seperately)\n",
    "bm25.retriever.index(corpus)\n",
    "\n",
    "triplets = []\n",
    "qids = list(qrels) \n",
    "hard_negatives_max = 10\n",
    "\n",
    "#### Retrieve BM25 hard negatives => Given a positive document, find most similar lexical documents\n",
    "for idx in tqdm.tqdm(range(len(qids)), desc=\"Retrieve Hard Negatives using BM25\"):\n",
    "    query_id, query_text = qids[idx], queries[qids[idx]]\n",
    "    pos_docs = [doc_id for doc_id in qrels[query_id] if qrels[query_id][doc_id] > 0]\n",
    "    pos_doc_texts = [corpus[doc_id][\"title\"] + \" \" + corpus[doc_id][\"text\"] for doc_id in pos_docs]\n",
    "    hits = bm25.retriever.es.lexical_multisearch(texts=pos_doc_texts, top_hits=hard_negatives_max+1)\n",
    "    for (pos_text, hit) in zip(pos_doc_texts, hits):\n",
    "        for (neg_id, _) in hit.get(\"hits\"):\n",
    "            if neg_id not in pos_docs:\n",
    "                neg_text = corpus[neg_id][\"title\"] + \" \" + corpus[neg_id][\"text\"]\n",
    "                triplets.append([query_text, pos_text, neg_text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ab/.pyenv/versions/3.11.6/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-28 20:48:11 - Use pytorch device_name: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### Provide any sentence-transformers or HF model\n",
    "model_name = \"distilbert-base-uncased\" \n",
    "word_embedding_model = models.Transformer(model_name, max_seq_length=300)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "#### Provide a high batch-size to train better with triplets!\n",
    "retriever = TrainRetriever(model=model, batch_size=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding Input Examples: 100%|██████████| 748/748 [00:00<00:00, 148534.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-28 20:48:14 - Loaded 8976 training pairs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### Prepare triplets samples\n",
    "train_samples = retriever.load_train_triplets(triplets=triplets)\n",
    "train_dataloader = retriever.prepare_train_triplets(train_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Training SBERT with cosine-product\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model=retriever.model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Prepare dev evaluator\n",
    "# ir_evaluator = retriever.load_ir_evaluator(dev_corpus, dev_queries, dev_qrels)\n",
    "\n",
    "#### If no dev set is present from above use dummy evaluator\n",
    "ir_evaluator = retriever.load_dummy_evaluator()\n",
    "\n",
    "#### Provide model save path\n",
    "model_save_path = os.path.join(os.getcwd(), \"output\", \"{}-v2-{}-bm25-hard-negs\".format(model_name, dataset))\n",
    "os.makedirs(model_save_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-28 20:48:15 - Starting to Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ab/.pyenv/versions/3.11.6/lib/python3.11/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='748' max='748' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [748/748 03:42, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Sequential Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>748</td>\n",
       "      <td>0.209600</td>\n",
       "      <td>No log</td>\n",
       "      <td>1716954720.268933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-28 20:52:00 - Save model to /mnt/c/D_drive/UCSD/Quarters/Q3/DSC253-Adv_txt_mining/Project/slm4search/test/output/distilbert-base-uncased-v2-scifact-bm25-hard-negs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#### Configure Train params\n",
    "num_epochs = 1\n",
    "evaluation_steps = 10000\n",
    "warmup_steps = int(len(train_samples) * num_epochs / retriever.batch_size * 0.1)\n",
    "\n",
    "retriever.fit(train_objectives=[(train_dataloader, train_loss)], \n",
    "                evaluator=ir_evaluator, \n",
    "                epochs=num_epochs,\n",
    "                output_path=model_save_path,\n",
    "                warmup_steps=warmup_steps,\n",
    "                evaluation_steps=evaluation_steps,\n",
    "                use_amp=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
