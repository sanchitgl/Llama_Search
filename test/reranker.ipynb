{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-27 23:44:10--  https://storage.googleapis.com/unicamp-dl/ia368dd_2023s1/dl20/topics.dl20.txt\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 2607:f8b0:4007:80f::201b, 2607:f8b0:4007:810::201b, 2607:f8b0:4007:803::201b, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|2607:f8b0:4007:80f::201b|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2276 (2.2K) [text/plain]\n",
      "Saving to: ‘topics.dl20.txt’\n",
      "\n",
      "topics.dl20.txt     100%[===================>]   2.22K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-05-27 23:44:10 (32.4 MB/s) - ‘topics.dl20.txt’ saved [2276/2276]\n",
      "\n",
      "--2024-05-27 23:44:10--  https://storage.googleapis.com/unicamp-dl/ia368dd_2023s1/dl20/qrels.dl20-passage.txt\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 2607:f8b0:4007:80f::201b, 2607:f8b0:4007:810::201b, 2607:f8b0:4007:811::201b, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|2607:f8b0:4007:80f::201b|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 218617 (213K) [text/plain]\n",
      "Saving to: ‘qrels.dl20-passage.txt’\n",
      "\n",
      "qrels.dl20-passage. 100%[===================>] 213.49K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2024-05-27 23:44:10 (13.3 MB/s) - ‘qrels.dl20-passage.txt’ saved [218617/218617]\n",
      "\n",
      "--2024-05-27 23:44:11--  https://storage.googleapis.com/unicamp-dl/ia368dd_2023s1/dl20/run.bm25.dl20.txt\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 2607:f8b0:4007:803::201b, 2607:f8b0:4007:811::201b, 2607:f8b0:4007:80f::201b, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|2607:f8b0:4007:803::201b|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2176763 (2.1M) [text/plain]\n",
      "Saving to: ‘run.bm25.dl20.txt’\n",
      "\n",
      "run.bm25.dl20.txt   100%[===================>]   2.08M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2024-05-27 23:44:11 (55.5 MB/s) - ‘run.bm25.dl20.txt’ saved [2176763/2176763]\n",
      "\n",
      "--2024-05-27 23:44:11--  https://storage.googleapis.com/unicamp-dl/ia368dd_2023s1/msmarco/collection.tsv\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 2607:f8b0:4007:810::201b, 2607:f8b0:4007:80f::201b, 2607:f8b0:4007:811::201b, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|2607:f8b0:4007:810::201b|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3061567852 (2.9G) [text/tab-separated-values]\n",
      "Saving to: ‘collection.tsv’\n",
      "\n",
      "collection.tsv      100%[===================>]   2.85G  64.5MB/s    in 48s     \n",
      "\n",
      "2024-05-27 23:45:00 (60.7 MB/s) - ‘collection.tsv’ saved [3061567852/3061567852]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://storage.googleapis.com/unicamp-dl/ia368dd_2023s1/dl20/topics.dl20.txt\n",
    "!wget https://storage.googleapis.com/unicamp-dl/ia368dd_2023s1/dl20/qrels.dl20-passage.txt\n",
    "!wget https://storage.googleapis.com/unicamp-dl/ia368dd_2023s1/dl20/run.bm25.dl20.txt\n",
    "!wget https://storage.googleapis.com/unicamp-dl/ia368dd_2023s1/msmarco/collection.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-27 23:45:21--  https://storage.googleapis.com/unicamp-dl/ia368dd_2023s1/msmarco/msmarco_triples.train.tiny.tsv\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 2607:f8b0:4007:803::201b, 2607:f8b0:4007:810::201b, 2607:f8b0:4007:80f::201b, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|2607:f8b0:4007:803::201b|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8076179 (7.7M) [text/tab-separated-values]\n",
      "Saving to: ‘msmarco_triples.train.tiny.tsv’\n",
      "\n",
      "msmarco_triples.tra 100%[===================>]   7.70M  49.3MB/s    in 0.2s    \n",
      "\n",
      "2024-05-27 23:45:22 (49.3 MB/s) - ‘msmarco_triples.train.tiny.tsv’ saved [8076179/8076179]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://storage.googleapis.com/unicamp-dl/ia368dd_2023s1/msmarco/msmarco_triples.train.tiny.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 27 23:52:29 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.113.01             Driver Version: 535.113.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   33C    P0              56W / 300W |      7MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          Off | 00000000:23:00.0 Off |                    0 |\n",
      "| N/A   34C    P0              73W / 300W |      7MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          Off | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   34C    P0              76W / 300W |     23MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          Off | 00000000:61:00.0 Off |                    0 |\n",
      "| N/A   52C    P0             175W / 300W |  26449MiB / 81920MiB |     43%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100 80GB PCIe          Off | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   40C    P0              80W / 300W |    449MiB / 81920MiB |      2%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA A100 80GB PCIe          Off | 00000000:A1:00.0 Off |                    0 |\n",
      "| N/A   58C    P0              96W / 300W |  48554MiB / 81920MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA A100 80GB PCIe          Off | 00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   62C    P0             224W / 300W |  32757MiB / 81920MiB |     87%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA A100 80GB PCIe          Off | 00000000:E1:00.0 Off |                    0 |\n",
      "| N/A   37C    P0              75W / 300W |    989MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    3   N/A  N/A   3562260      C   python                                    26436MiB |\n",
      "|    4   N/A  N/A   4129252      C   ...nshu/miniconda3/envs/VQA/bin/python      436MiB |\n",
      "|    5   N/A  N/A   4096055      C   ...h/anaconda3/envs/lm-eval/bin/python    32744MiB |\n",
      "|    5   N/A  N/A   4122249      C   ...forge3/envs/semantic-sam/bin/python    15792MiB |\n",
      "|    6   N/A  N/A   4096056      C   ...h/anaconda3/envs/lm-eval/bin/python    32744MiB |\n",
      "|    7   N/A  N/A   4101194      C   python                                      976MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset size: x-22000, y-22000\n",
      "20000 training examples.\n",
      "2000 valid examples.\n",
      "\n",
      "First 3 training examples:\n",
      "Label: True Input: \"Query: is a little caffeine ok during pregnancy Passage: We donât know a lot about the effects of caffeine during pregnancy on you and your baby. So itâs best to limit the amount you get each day. If youâre pregnant, limit caffeine to 200 milligrams each day. This is about the amount in 1Â½ 8-ounce cups of coffee or one 12-ounce cup of coffee.\"\n",
      "Label: False Input: \"Query: is a little caffeine ok during pregnancy Passage: It is generally safe for pregnant women to eat chocolate because studies have shown to prove certain benefits of eating chocolate during pregnancy. However, pregnant women should ensure their caffeine intake is below 200 mg per day.\"\n",
      "Label: True Input: \"Query: what fruit is native to australia Passage: Passiflora herbertiana. A rare passion fruit native to Australia. Fruits are green-skinned, white fleshed, with an unknown edible rating. Some sources list the fruit as edible, sweet and tasty, while others list the fruits as being bitter and inedible.assiflora herbertiana. A rare passion fruit native to Australia. Fruits are green-skinned, white fleshed, with an unknown edible rating. Some sources list the fruit as edible, sweet and tasty, while others list the fruits as being bitter and inedible.\"\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "First 3 validation examples:\n",
      "Label: True Input: \"Query: how many children does emily deschanel have Passage: Emily Deschanel is not currently pregnant. Emily Deschanel and her husband David Hornsby announced on March 31, 2011 that they were expecting their first child together. Desc â¦ hanel gave birth in September 2011.\"\n",
      "Label: False Input: \"Query: how many children does emily deschanel have Passage: (film) Equilibrium is a 2002 American dystopian science fiction film written and directed by Kurt Wimmer and starring Christian Bale, Emily Watson, and Taye Diggs.\"\n",
      "Label: True Input: \"Query: what is venous Passage: Venous blood. Venous blood is deoxygenated blood which travels from the peripheral vessels, through the venous system into the right atrium of the heart. Deoxygenated blood is then pumped by the right ventricle to the lungs via the pulmonary artery which is divided in two branches, left and right to the left and right lungs respectively. Blood is oxygenated in the lungs and returns to the left atrium through the pulmonary veins.\"\n"
     ]
    }
   ],
   "source": [
    "max_train = 20_000\n",
    "max_valid = 2_000\n",
    "\n",
    "def serialize_query_passage(query, passage):\n",
    "    return f'Query: {query} Passage: {passage}'\n",
    "\n",
    "\n",
    "def load_examples(path):\n",
    "    x = []\n",
    "    y = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            query, positive_passage, negative_passage = line.strip().split('\\t')\n",
    "            x.append(serialize_query_passage(query, positive_passage))\n",
    "            y.append(True)\n",
    "            x.append(serialize_query_passage(query, negative_passage))\n",
    "            y.append(False)\n",
    "    return x, y\n",
    "\n",
    "x_train, y_train = load_examples('msmarco_triples.train.tiny.tsv')\n",
    "\n",
    "print(f\"Loaded dataset size: x-{len(x_train)}, y-{len(y_train)}\")\n",
    "\n",
    "x_valid = x_train[-max_valid:]\n",
    "y_valid = y_train[-max_valid:]\n",
    "x_train = x_train[:max_train]\n",
    "y_train = y_train[:max_train]\n",
    "\n",
    "print(len(x_train), 'training examples.')\n",
    "print(len(x_valid), 'valid examples.')\n",
    "\n",
    "print('\\nFirst 3 training examples:')\n",
    "for x, y in zip(x_train[:3], y_train[:3]):\n",
    "    print(f'Label: {y} Input: \"{x}\"')\n",
    "\n",
    "print('\\n' + 100 * '-' + '\\n\\nFirst 3 validation examples:')\n",
    "for x, y in zip(x_valid[:3], y_valid[:3]):\n",
    "    print(f'Label: {y} Input: \"{x}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/addullah/miniconda3/envs/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/addullah/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/home/addullah/miniconda3/envs/myenv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = 'microsoft/MiniLM-L12-H384-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length in tokens: 89.90\n",
      "Stdev length in tokens: 33.08\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean, stdev\n",
    "\n",
    "lengths = [len(tokens) for tokens in tokenizer(x_train[:1_000])['input_ids']]\n",
    "print(f'Mean length in tokens: {mean(lengths):0.2f}')\n",
    "print(f'Stdev length in tokens: {stdev(lengths):0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the mean passage length is ~90 tokens, a maximum of 250 tokens should be more than enough to acommodate\n",
    "# query+passage tokens in the input.\n",
    "max_length = 300\n",
    "x_train_tokenized = tokenizer(x_train, max_length=max_length, truncation=True)\n",
    "x_valid_tokenized = tokenizer(x_valid, max_length=max_length, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, examples, targets=None):\n",
    "        self.examples = examples\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples['input_ids'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        label = 0\n",
    "        if self.targets is not None:\n",
    "            label = int(self.targets[idx])\n",
    "                \n",
    "        return {\n",
    "            'input_ids': self.examples['input_ids'][idx],\n",
    "            'attention_mask': self.examples['attention_mask'][idx],\n",
    "            'labels': label,\n",
    "        }\n",
    "\n",
    "dataset_train = Dataset(x_train_tokenized, y_train)\n",
    "assert len(dataset_train[0]['input_ids']) > 0\n",
    "assert len(dataset_train[1]['attention_mask']) > 0\n",
    "assert type(dataset_train[2]['labels']) == int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Dataset(x_train_tokenized, y_train)\n",
    "dataset_valid = Dataset(x_valid_tokenized, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BatchEncoding\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return BatchEncoding(tokenizer.pad(batch, return_tensors='pt'))\n",
    "\n",
    "dataloader_train = data.DataLoader(dataset_train, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "dataloader_valid = data.DataLoader(dataset_valid, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "for batch in dataloader_train:\n",
    "    assert batch['input_ids'].shape[0] <= dataloader_train.batch_size\n",
    "    assert batch['input_ids'].shape[1] <= max_length\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first define an evaluation function to measure accuracy and loss.\n",
    "# It is not common to use accuracy to evaluate IR models, but it is expensive to run the reranking task in the training\n",
    "# loop.\n",
    "\n",
    "def evaluate(model, dataloader, set_name):\n",
    "    losses = []\n",
    "    correct = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, mininterval=0.5, desc=set_name, disable=False):\n",
    "            outputs = model(**batch.to(device))\n",
    "            loss_val = outputs.loss\n",
    "            losses.append(loss_val.cpu().item())\n",
    "            preds = outputs.logits.argmax(dim=1)\n",
    "            correct += (preds == batch['labels']).sum().item()\n",
    "\n",
    "    print(f'{set_name} loss: {mean(losses):0.3f}; {set_name} accuracy: {correct / len(dataloader.dataset):0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/MiniLM-L12-H384-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters 33360770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid: 100%|██████████| 63/63 [00:01<00:00, 35.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.693; Valid accuracy: 0.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 625/625 [00:50<00:00, 12.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Training loss: 0.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid: 100%|██████████| 63/63 [00:01<00:00, 45.01it/s]\n",
      "Epochs:  50%|█████     | 1/2 [00:52<00:52, 52.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.263; Valid accuracy: 0.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 625/625 [00:43<00:00, 14.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Training loss: 0.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid: 100%|██████████| 63/63 [00:01<00:00, 44.91it/s]\n",
      "Epochs: 100%|██████████| 2/2 [01:36<00:00, 48.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.240; Valid accuracy: 0.911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "print('Parameters', model.num_parameters())\n",
    "\n",
    "epochs = 2\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "num_training_steps = epochs * len(dataloader_train)\n",
    "num_warmup_steps = int(num_training_steps * 0.1)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "\n",
    "# First validation to check if evaluation code is working and accuracy is random as expected \n",
    "evaluate(model=model, dataloader=dataloader_valid, set_name='Valid')\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc='Epochs'):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for batch in tqdm(dataloader_train, mininterval=0.5, desc='Train', disable=False):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch.to(device))\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_losses.append(loss.cpu().item())\n",
    "\n",
    "    print(f'Epoch: {epoch + 1} Training loss: {mean(train_losses):0.2f}')\n",
    "    evaluate(model=model, dataloader=dataloader_valid, set_name='Valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating on TREC-DL 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "\n",
    "def load_queries(path):\n",
    "    \"\"\"Returns a dictionary whose keys are query ids and values are query texts.\"\"\"\n",
    "    queries = {}\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            query_id, query_text = line.strip().split('\\t')\n",
    "            queries[query_id] = query_text\n",
    "    return queries\n",
    "\n",
    "\n",
    "def load_run(path):\n",
    "    \"\"\"Returns a dictionary whose keys are query ids and values are lists of passage ids.\"\"\"\n",
    "    run = collections.defaultdict(list)\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            query_id, _, passage_id, _, _, _ = line.strip().split()\n",
    "            run[query_id].append(passage_id)\n",
    "    return run\n",
    "\n",
    "\n",
    "def load_corpus(path):\n",
    "    \"\"\"Returns a dictionary whose keys are passage ids and values are passage texts.\"\"\"\n",
    "    corpus = {}\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            passage_id, passage_text = line.strip().split('\\t')\n",
    "            corpus[passage_id] = passage_text\n",
    "    return corpus\n",
    "\n",
    "\n",
    "queries = load_queries(path='topics.dl20.txt')\n",
    "run = load_run(path='run.bm25.dl20.txt')\n",
    "corpus = load_corpus(path='collection.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_run_to_examples(run, corpus, queries):\n",
    "    \"\"\"Returns:\n",
    "        - input_examples: a list of query-pairs pairs serialized in the same format used during finetuning.\n",
    "        - query_passage_ids: a list of query-passage ids aligned with input_examples.\n",
    "    \"\"\"\n",
    "\n",
    "    input_examples = []\n",
    "    query_passage_ids = []\n",
    "    for query_id, passage_ids in run.items():\n",
    "        for passage_id in passage_ids:\n",
    "            query_text = queries[query_id]\n",
    "            passage_text = corpus[passage_id]\n",
    "            input_text = serialize_query_passage(query_text, passage_text)\n",
    "            input_examples.append(input_text)\n",
    "            query_passage_ids.append((query_id, passage_id))\n",
    "    return input_examples, query_passage_ids\n",
    "\n",
    "\n",
    "x_test, query_passage_ids = convert_run_to_examples(run=run, corpus=corpus, queries=queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert examples to batches\n",
    "x_test_tokenized = tokenizer(x_test, max_length=max_length, truncation=True)\n",
    "dataset_test = Dataset(x_test_tokenized, targets=None)\n",
    "dataloader_test = data.DataLoader(dataset_test, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now predict relevance scores for each query-passage pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 1688/1688 [00:38<00:00, 43.93it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    scores = []\n",
    "    for batch in tqdm(dataloader_test, mininterval=0.5, desc='Test', disable=False):\n",
    "        outputs = model(**batch.to(device))\n",
    "        scores.extend(outputs.logits[:, 1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now convert the results to a \"run\" object in the format expected by pytrec_eval.\n",
    "assert len(query_passage_ids) == len(scores)\n",
    "\n",
    "run = collections.defaultdict(dict)\n",
    "for (query_id, passage_id), score in zip(query_passage_ids, scores):\n",
    "    run[query_id][passage_id] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54000, 54000)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scores), len(query_passage_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nDCG@10: 0.6126\n"
     ]
    }
   ],
   "source": [
    "import pytrec_eval\n",
    "\n",
    "with open('qrels.dl20-passage.txt', 'r') as f_qrel:\n",
    "    qrels = pytrec_eval.parse_qrel(f_qrel)\n",
    "\n",
    "metric = 'ndcg_cut_10'\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrels, {metric})\n",
    "results = evaluator.evaluate(run)\n",
    "print(f'nDCG@10: {sum(item[metric] for item in results.values()) / len(results):0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qrels\n",
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recip_rank: 0.8534\n"
     ]
    }
   ],
   "source": [
    "# import pytrec_eval\n",
    "\n",
    "# with open('qrels.dl20-passage.txt', 'r') as f_qrel:\n",
    "#     qrels = pytrec_eval.parse_qrel(f_qrel)\n",
    "\n",
    "metric = 'recip_rank'\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrels, {metric})\n",
    "results = evaluator.evaluate(run)\n",
    "print(f'recip_rank: {sum(item[metric] for item in results.values()) / len(results):0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'11pt_avg',\n",
       " 'G',\n",
       " 'P',\n",
       " 'Rndcg',\n",
       " 'Rprec',\n",
       " 'Rprec_mult',\n",
       " 'binG',\n",
       " 'bpref',\n",
       " 'gm_bpref',\n",
       " 'gm_map',\n",
       " 'infAP',\n",
       " 'iprec_at_recall',\n",
       " 'map',\n",
       " 'map_cut',\n",
       " 'ndcg',\n",
       " 'ndcg_cut',\n",
       " 'ndcg_rel',\n",
       " 'num_nonrel_judged_ret',\n",
       " 'num_q',\n",
       " 'num_rel',\n",
       " 'num_rel_ret',\n",
       " 'num_ret',\n",
       " 'recall',\n",
       " 'recip_rank',\n",
       " 'relative_P',\n",
       " 'relstring',\n",
       " 'runid',\n",
       " 'set_F',\n",
       " 'set_P',\n",
       " 'set_map',\n",
       " 'set_recall',\n",
       " 'set_relative_P',\n",
       " 'success',\n",
       " 'utility'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytrec_eval.supported_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
